{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=2, b=3\n",
      "Addition with constants: 5\n",
      "Multiplication with constants: 6\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "with tf.Session() as sess:\n",
    "    print(\"a=2, b=3\")\n",
    "    print(\"Addition with constants: %i\" % sess.run(a+b))\n",
    "    print(\"Multiplication with constants: %i\" % sess.run(a*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition with variables: 5\n",
      "Multiplication with variables: 6\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)\n",
    "add = tf.add(a, b)\n",
    "mul = tf.multiply(a, b)\n",
    "with tf.Session() as sess:\n",
    "    # Run every operation with variable input\n",
    "    print(\"Addition with variables: %i\" % sess.run(add, feed_dict={a: 2, b: 3}))\n",
    "    print(\"Multiplication with variables: %i\" % sess.run(mul, feed_dict={a: 2, b: 3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.]]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 2000\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = tf.add(tf.multiply(X, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_sum(tf.pow(activation-Y, 2))/(2*n_samples) #L2 loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) #Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.272730231 W= 0.5131217 b= -0.95956266\n",
      "Epoch: 0051 cost= 0.247118503 W= 0.48017073 b= -0.85728914\n",
      "Epoch: 0101 cost= 0.227464363 W= 0.46645713 b= -0.7586346\n",
      "Epoch: 0151 cost= 0.210079893 W= 0.4535594 b= -0.66584873\n",
      "Epoch: 0201 cost= 0.194702774 W= 0.44142854 b= -0.57858074\n",
      "Epoch: 0251 cost= 0.181101486 W= 0.43001932 b= -0.49650347\n",
      "Epoch: 0301 cost= 0.169070825 W= 0.41928864 b= -0.41930747\n",
      "Epoch: 0351 cost= 0.158429444 W= 0.40919605 b= -0.34670192\n",
      "Epoch: 0401 cost= 0.149017066 W= 0.39970362 b= -0.2784146\n",
      "Epoch: 0451 cost= 0.140691832 W= 0.39077586 b= -0.21418886\n",
      "Epoch: 0501 cost= 0.133328184 W= 0.38237906 b= -0.15378313\n",
      "Epoch: 0551 cost= 0.126815021 W= 0.3744817 b= -0.0969698\n",
      "Epoch: 0601 cost= 0.121054284 W= 0.36705405 b= -0.04353563\n",
      "Epoch: 0651 cost= 0.115959018 W= 0.36006808 b= 0.00672068\n",
      "Epoch: 0701 cost= 0.111452401 W= 0.35349762 b= 0.05398798\n",
      "Epoch: 0751 cost= 0.107466415 W= 0.34731796 b= 0.09844411\n",
      "Epoch: 0801 cost= 0.103940994 W= 0.34150583 b= 0.14025612\n",
      "Epoch: 0851 cost= 0.100822933 W= 0.3360393 b= 0.1795816\n",
      "Epoch: 0901 cost= 0.098065175 W= 0.33089805 b= 0.21656823\n",
      "Epoch: 0951 cost= 0.095626123 W= 0.32606247 b= 0.2513551\n",
      "Epoch: 1001 cost= 0.093469009 W= 0.32151437 b= 0.28407285\n",
      "Epoch: 1051 cost= 0.091561176 W= 0.3172369 b= 0.31484511\n",
      "Epoch: 1101 cost= 0.089873955 W= 0.31321397 b= 0.34378624\n",
      "Epoch: 1151 cost= 0.088381782 W= 0.30943015 b= 0.37100664\n",
      "Epoch: 1201 cost= 0.087062091 W= 0.30587125 b= 0.3966089\n",
      "Epoch: 1251 cost= 0.085895017 W= 0.3025242 b= 0.42068732\n",
      "Epoch: 1301 cost= 0.084862955 W= 0.29937613 b= 0.44333428\n",
      "Epoch: 1351 cost= 0.083950229 W= 0.29641524 b= 0.46463442\n",
      "Epoch: 1401 cost= 0.083143078 W= 0.29363054 b= 0.4846681\n",
      "Epoch: 1451 cost= 0.082429409 W= 0.2910116 b= 0.50350857\n",
      "Epoch: 1501 cost= 0.081798285 W= 0.28854835 b= 0.5212286\n",
      "Epoch: 1551 cost= 0.081240192 W= 0.28623155 b= 0.53789574\n",
      "Epoch: 1601 cost= 0.080746688 W= 0.28405243 b= 0.55357194\n",
      "Epoch: 1651 cost= 0.080310322 W= 0.28200302 b= 0.5683158\n",
      "Epoch: 1701 cost= 0.079924472 W= 0.2800753 b= 0.5821835\n",
      "Epoch: 1751 cost= 0.079583295 W= 0.27826226 b= 0.59522635\n",
      "Epoch: 1801 cost= 0.079281680 W= 0.27655694 b= 0.6074941\n",
      "Epoch: 1851 cost= 0.079015009 W= 0.2749534 b= 0.6190304\n",
      "Epoch: 1901 cost= 0.078779303 W= 0.27344525 b= 0.6298795\n",
      "Epoch: 1951 cost= 0.078570910 W= 0.27202678 b= 0.64008373\n",
      "Optimization Finished!\n",
      "cost= 0.07839013 W= 0.27071857 b= 0.6494951\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(sess.run(cost, feed_dict={X: train_X, Y:train_Y})), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"cost=\", sess.run(cost, feed_dict={X: train_X, Y: train_Y}), \\\n",
    "          \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "#the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/zhangchunyu/Downloads/projects/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/zhangchunyu/Downloads/projects/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /Users/zhangchunyu/Downloads/projects/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/zhangchunyu/Downloads/projects/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/Users/zhangchunyu/Downloads/projects/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "# Cross entropy\n",
    "cost = -tf.reduce_sum(y*tf.log(activation)) \n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 29.973840472\n",
      "Epoch: 0002 cost= 21.943272914\n",
      "Epoch: 0003 cost= 21.056873987\n",
      "Epoch: 0004 cost= 20.620348063\n",
      "Epoch: 0005 cost= 20.141774274\n",
      "Epoch: 0006 cost= 19.859094145\n",
      "Epoch: 0007 cost= 19.766261680\n",
      "Epoch: 0008 cost= 19.559928859\n",
      "Epoch: 0009 cost= 19.289826989\n",
      "Epoch: 0010 cost= 19.133579881\n",
      "Epoch: 0011 cost= 19.127393136\n",
      "Epoch: 0012 cost= 18.987054520\n",
      "Epoch: 0013 cost= 18.919631242\n",
      "Epoch: 0014 cost= 18.911849411\n",
      "Epoch: 0015 cost= 18.839719786\n",
      "Epoch: 0016 cost= 18.632711704\n",
      "Epoch: 0017 cost= 18.571200793\n",
      "Epoch: 0018 cost= 18.622702306\n",
      "Epoch: 0019 cost= 18.489630599\n",
      "Epoch: 0020 cost= 18.509504071\n",
      "Epoch: 0021 cost= 18.453594574\n",
      "Epoch: 0022 cost= 18.460840832\n",
      "Epoch: 0023 cost= 18.303117810\n",
      "Epoch: 0024 cost= 18.267246979\n",
      "Epoch: 0025 cost= 18.353595890\n",
      "Optimization Finished!\n",
      "Accuracy: 0.922\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############alex net\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 300000\n",
    "batch_size = 64\n",
    "display_step = 100\n",
    "\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.8 # Dropout, probability to keep units\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], \n",
    "                                                  padding='SAME'),b), name=name)\n",
    "\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], \n",
    "                          padding='SAME', name=name)\n",
    "\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
    "\n",
    "def alex_net(_X, _weights, _biases, _dropout):\n",
    "    # Reshape input picture\n",
    "    _X = tf.reshape(_X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool1 = max_pool('pool1', conv1, k=2)\n",
    "    # Apply Normalization\n",
    "    norm1 = norm('norm1', pool1, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool2 = max_pool('pool2', conv2, k=2)\n",
    "    # Apply Normalization\n",
    "    norm2 = norm('norm2', pool2, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    # Apply Normalization\n",
    "    norm3 = norm('norm3', pool3, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv3 output to fit dense layer input\n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    # Relu activation\n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1')\n",
    "    \n",
    "    # Relu activation\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') \n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),\n",
    "    'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])),\n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([64])),\n",
    "    'bc2': tf.Variable(tf.random_normal([128])),\n",
    "    'bc3': tf.Variable(tf.random_normal([256])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = alex_net(x, weights, biases, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6400, Minibatch Loss= 34276.148438, Training Accuracy= 0.56250\n",
      "Iter 12800, Minibatch Loss= 19393.035156, Training Accuracy= 0.62500\n",
      "Iter 19200, Minibatch Loss= 17423.998047, Training Accuracy= 0.73438\n",
      "Iter 25600, Minibatch Loss= 11945.751953, Training Accuracy= 0.79688\n",
      "Iter 32000, Minibatch Loss= 6998.734375, Training Accuracy= 0.85938\n",
      "Iter 38400, Minibatch Loss= 2915.044678, Training Accuracy= 0.87500\n",
      "Iter 44800, Minibatch Loss= 8352.572266, Training Accuracy= 0.85938\n",
      "Iter 51200, Minibatch Loss= 4030.079590, Training Accuracy= 0.84375\n",
      "Iter 57600, Minibatch Loss= 3487.095215, Training Accuracy= 0.87500\n",
      "Iter 64000, Minibatch Loss= 2927.845215, Training Accuracy= 0.85938\n",
      "Iter 70400, Minibatch Loss= 1162.272461, Training Accuracy= 0.93750\n",
      "Iter 76800, Minibatch Loss= 4121.847168, Training Accuracy= 0.82812\n",
      "Iter 83200, Minibatch Loss= 4033.260254, Training Accuracy= 0.89062\n",
      "Iter 89600, Minibatch Loss= 4822.212402, Training Accuracy= 0.87500\n",
      "Iter 96000, Minibatch Loss= 1462.036133, Training Accuracy= 0.89062\n",
      "Iter 102400, Minibatch Loss= 2734.202393, Training Accuracy= 0.89062\n",
      "Iter 108800, Minibatch Loss= 5408.774414, Training Accuracy= 0.87500\n",
      "Iter 115200, Minibatch Loss= 2100.406738, Training Accuracy= 0.89062\n",
      "Iter 121600, Minibatch Loss= 736.526917, Training Accuracy= 0.98438\n",
      "Iter 128000, Minibatch Loss= 2329.972168, Training Accuracy= 0.90625\n",
      "Iter 134400, Minibatch Loss= 1358.259644, Training Accuracy= 0.93750\n",
      "Iter 140800, Minibatch Loss= 2963.087891, Training Accuracy= 0.89062\n",
      "Iter 147200, Minibatch Loss= 738.792236, Training Accuracy= 0.98438\n",
      "Iter 153600, Minibatch Loss= 1093.777710, Training Accuracy= 0.90625\n",
      "Iter 160000, Minibatch Loss= 332.829041, Training Accuracy= 0.96875\n",
      "Iter 166400, Minibatch Loss= 6301.456055, Training Accuracy= 0.87500\n",
      "Iter 172800, Minibatch Loss= 130.158142, Training Accuracy= 0.96875\n",
      "Iter 179200, Minibatch Loss= 2050.000732, Training Accuracy= 0.92188\n",
      "Iter 185600, Minibatch Loss= 2955.307129, Training Accuracy= 0.92188\n",
      "Iter 192000, Minibatch Loss= 1717.436035, Training Accuracy= 0.90625\n",
      "Iter 198400, Minibatch Loss= 1146.356689, Training Accuracy= 0.96875\n",
      "Iter 204800, Minibatch Loss= 1043.174805, Training Accuracy= 0.95312\n",
      "Iter 211200, Minibatch Loss= 2813.330566, Training Accuracy= 0.92188\n",
      "Iter 217600, Minibatch Loss= 492.937256, Training Accuracy= 0.96875\n",
      "Iter 224000, Minibatch Loss= 1303.062866, Training Accuracy= 0.90625\n",
      "Iter 230400, Minibatch Loss= 3206.026367, Training Accuracy= 0.84375\n",
      "Iter 236800, Minibatch Loss= 1116.028320, Training Accuracy= 0.96875\n",
      "Iter 243200, Minibatch Loss= 1371.504517, Training Accuracy= 0.90625\n",
      "Iter 249600, Minibatch Loss= 497.974731, Training Accuracy= 0.95312\n",
      "Iter 256000, Minibatch Loss= 1591.293701, Training Accuracy= 0.92188\n",
      "Iter 262400, Minibatch Loss= 336.632751, Training Accuracy= 0.93750\n",
      "Iter 268800, Minibatch Loss= 652.945435, Training Accuracy= 0.95312\n",
      "Iter 275200, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 281600, Minibatch Loss= 960.439758, Training Accuracy= 0.93750\n",
      "Iter 288000, Minibatch Loss= 230.878967, Training Accuracy= 0.96875\n",
      "Iter 294400, Minibatch Loss= 395.267639, Training Accuracy= 0.98438\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9765625\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" \\\n",
    "                  + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], \n",
    "                                                             y: mnist.test.labels[:256], \n",
    "                                                             keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "########a lstm character model over text8 data\n",
    "import urllib\n",
    "import zipfile\n",
    "import random\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8548920\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to verify text8.zip. Can you get to it with a browser?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-c25377bef152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text8.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31344016\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-c25377bef152>\u001b[0m in \u001b[0;36mmaybe_download\u001b[0;34m(filename, expected_bytes)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     raise Exception(\n\u001b[0;32m---> 13\u001b[0;31m       'Failed to verify ' + filename + '. Can you get to it with a browser?')\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Failed to verify text8.zip. Can you get to it with a browser?"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download(\"text8.zip\", 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "filename = \"/Users/zhangchunyu/Downloads/projects/text8.zip\"\n",
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return f.read(name)\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print(\"Data size\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 b'ons anarchists advocate social relations based upon voluntary as'\n",
      "1000 b' anarchism originated as a term of abuse first used against earl'\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "(1, 26, 0, 0)\n",
      "('a', 'z', ' ')\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character:', char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print((char2id('a'), char2id('z'), char2id(' '), char2id('ï')))\n",
    "print((id2char(1), id2char(26), id2char(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', 'abbeys and ', 'arried urra', 'el and rich', 'and liturgi', ' opened for', 'n from the ', 'ration took', ' york other', 'oeing seven', 'sted with a', ' has probab', 'made to rec', 'ho received', 'gnificant t', 'ce critic o', 'ix eight in', 'le s uncaus', ' be lost as', 'tracellular', 'the size of', 'ss him a st', ' drugs conf', 'l take to c', 'e the pries', 'im to name ', 'd barred at', 'standard fo', 'such as eso', 'e on the gr', 'of the orig', 'hiver one n', 'eight march', ' lead chara', 'classical m', 'the non gm ', 'nalysis fun', 'ons believe', 'at least no', 'greed upon ', 'ystem examp', ' based on t', 'es the offi', 'ission at t', ' nine three', 'e linux ent', 'st daily co', 'ntration ca', ' nehru wish', ' stiff from', 'harman s sy', 'to to begin', 'itiatives t', 'these autho', 'cky ricardo', ' of mathema', 'nt of arm i', 'edited prog', 'external li', 'ther state ', 'ddhism espe', 'es possible']\n",
      "['ists advoca', 'ary governm', 'hes nationa', ' monasterie', 'aca princes', 'hard baer h', 'ical langua', 'r passenger', ' national m', 'k place dur', 'r well know', 'n six seven', 'a gloss cov', 'bly been on', 'cognize sin', 'd the first', 'than in jer', 'of the pove', 'n signs of ', 'sed cause s', 's in denatu', 'r ice forma', 'f the input', 'tick to pul', 'fusion inab', 'complete an', 'st of the m', ' it fort de', 'ttempts by ', 'ormats for ', 'oteric chri', 'rowing popu', 'ginal docum', 'nine eight ', 'h eight lis', 'acter lieut', 'mechanics a', ' comparison', 'ndamental a', 'e the confi', 'ot parliame', ' by histori', 'ple rlc cir', 'the whole g', 'icial langu', 'this point ', 'e two one o', 'terprise se', 'ollege news', 'amp lewis h', 'hed the eco', 'm flat to t', 'ydney based', 'n negotiati', 'the lesotho', 'ors wrote i', 'o this clas', 'atics prese', 'is represen', 'grams must ', 'inks bbc on', ' modern day', 'ecially rep', 'e the syste']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "import numpy as np\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size / batch_size\n",
    "    self._cursor = [ int(offset * segment) for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      char_tmp = self._text[self._cursor[b]:self._cursor[b]+1]\n",
    "      #print(b,self._cursor[b],char_tmp)\n",
    "      batch[b, char2id(char_tmp.decode())] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat:0\", shape=(640, 64), dtype=float32) (64, 27) (27,)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    print(tf.concat(outputs,0),w.shape,b.shape)\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "         labels=tf.concat(train_labels,0),logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.293534755706787 learning rate: 10.0\n",
      "Minibatch perplexity: 26.94\n",
      "================================================================================\n",
      "yxa etecpdfdseqyfmlefnjjaqu oinvs cenbem jvggexeahsz lnooa nntueoud ete sa  evcn\n",
      "ppgfmkqb  tcsnyioduse eqgiesesap qvm scu xbsf  fvrwc  lbgnyknkjqjjrifpeaiam nmyl\n",
      "oo ipnrtmkib nsospv n eqfhildamliuesywnnspnh b   brhyliuagdlt ut om aaaooljppt g\n",
      "zatacca eqaolqknjgsdaeuszeidsirzsilagxjg gsatpqq o ttvcwtjg qdidv qir by pjntmqa\n",
      "qmydxonccl gq ioor rvic   aop f  iypjb titihceenl aaollillst ragkhcmciicslseferr\n",
      "================================================================================\n",
      "Validation set perplexity: 20.10\n",
      "Average loss at step 100 : 2.5868499302864074 learning rate: 10.0\n",
      "Minibatch perplexity: 10.11\n",
      "Validation set perplexity: 11.29\n",
      "Average loss at step 200 : 2.2547388815879823 learning rate: 10.0\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 300 : 2.0982280242443085 learning rate: 10.0\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 400 : 1.9846902918815612 learning rate: 10.0\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 500 : 1.9299036109447478 learning rate: 10.0\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600 : 1.9088555657863617 learning rate: 10.0\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 700 : 1.849248013496399 learning rate: 10.0\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 800 : 1.8153596723079681 learning rate: 10.0\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 900 : 1.8308302354812622 learning rate: 10.0\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1000 : 1.8184204816818237 learning rate: 10.0\n",
      "Minibatch perplexity: 5.50\n",
      "================================================================================\n",
      "qut trams of the in absouct were consingricy to the vift free still to be is as \n",
      "ustion h darkrays there hestelwectlationals twind elade eemer decals oglie one t\n",
      "ver the joold one four five zero zero time secthas wall the denation and pr on t\n",
      "cirito azorimi decon groms persinustive the collion of transf of the gnic confri\n",
      "willion mainted ethes the ut hisestion beem thim hding losipte attwown and the o\n",
      "================================================================================\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1100 : 1.771464818716049 learning rate: 10.0\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1200 : 1.753631398677826 learning rate: 10.0\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1300 : 1.7246283459663392 learning rate: 10.0\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1400 : 1.7396213436126708 learning rate: 10.0\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1500 : 1.7277963507175444 learning rate: 10.0\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1600 : 1.7450848019123077 learning rate: 10.0\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700 : 1.7092740046977997 learning rate: 10.0\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1800 : 1.6694533181190492 learning rate: 10.0\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 1900 : 1.6476267969608307 learning rate: 10.0\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2000 : 1.6917737340927124 learning rate: 10.0\n",
      "Minibatch perplexity: 6.05\n",
      "================================================================================\n",
      "m wetter j s rowendary eas for the quild a hump hest bezaka crolaction selt obn \n",
      "x films live onfium of the humall fames of his exothork asism laby was s world b\n",
      "u or meptom is is use and gone that electing foursht hap yer time three spo afti\n",
      "chinay usocua them intern for carrbotory uncop one to pat alse abo be marthelgn \n",
      "fin of dec real catolly he sounce larn concentriv heas monts to the was checord \n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2100 : 1.6848526394367218 learning rate: 10.0\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2200 : 1.6740488350391387 learning rate: 10.0\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2300 : 1.6369680655002594 learning rate: 10.0\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2400 : 1.6547150027751922 learning rate: 10.0\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2500 : 1.6718779289722443 learning rate: 10.0\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2600 : 1.6484804987907409 learning rate: 10.0\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2700 : 1.646898101568222 learning rate: 10.0\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2800 : 1.639953327178955 learning rate: 10.0\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2900 : 1.6502558124065398 learning rate: 10.0\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3000 : 1.6444936203956604 learning rate: 10.0\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "que of economics as esudal mearnsiu telens in engld of les gornerdation one nine\n",
      "s vociated was the fame protiment drmbers stice that nor the borme to a all the \n",
      "vantly six three nine reball same instated are horm while woul form the nine sxi\n",
      "gnery is which as a person ont teresixed as a five arrid to folue the tloxmentts\n",
      "e compation state the uubusian rata a may times a mone abralt one six he his ats\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3100 : 1.6278613555431365 learning rate: 10.0\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3200 : 1.6392591381072998 learning rate: 10.0\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3300 : 1.635470758676529 learning rate: 10.0\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3400 : 1.6620840811729432 learning rate: 10.0\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3500 : 1.6495033276081086 learning rate: 10.0\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3600 : 1.6552410840988159 learning rate: 10.0\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3700 : 1.6439146506786346 learning rate: 10.0\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3800 : 1.63324458360672 learning rate: 10.0\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3900 : 1.6330500471591949 learning rate: 10.0\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4000 : 1.6464191126823424 learning rate: 10.0\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "x christion to on wayly sovery paprach can gorld anarcholls the emperem poalted \n",
      "x and sead but of callate grack at ghatne these the were od the siani that in ag\n",
      "gallum thit more two six repurin ortonthi agogn convaniul singed and the couct o\n",
      "work the five i s but his year center dol adrain and noats american mate halbe a\n",
      "got con erul pactation of confitsis not camble galys to scientinglia deficiation\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4100 : 1.6314809918403625 learning rate: 10.0\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4200 : 1.6280662024021149 learning rate: 10.0\n",
      "Minibatch perplexity: 4.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4300 : 1.611699321269989 learning rate: 10.0\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4400 : 1.6048918795585632 learning rate: 10.0\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4500 : 1.6153822469711303 learning rate: 10.0\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4600 : 1.6112724506855012 learning rate: 10.0\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4700 : 1.6197839021682738 learning rate: 10.0\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4800 : 1.6280920910835266 learning rate: 10.0\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4900 : 1.6292267894744874 learning rate: 10.0\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000 : 1.6043220698833465 learning rate: 1.0\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "jame of a reple oney onth which a hadfthops avony comminate in nystage not than \n",
      " in the onem of the certion deshingand in the are sivel conferent of itas follow\n",
      "fice by gaisen to may one eight on jreath anythel one new roman in the blackenta\n",
      "s we grain yepine sants the yeath capoular about the damica in sema five zero se\n",
      "le suicia peapes kimzer relal in an eccustic a coaciled on a laborals of to be a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5100 : 1.5967319011688232 learning rate: 1.0\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5200 : 1.586120228767395 learning rate: 1.0\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5300 : 1.574183735847473 learning rate: 1.0\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5400 : 1.570448956489563 learning rate: 1.0\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5500 : 1.5632037329673767 learning rate: 1.0\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600 : 1.5793245792388917 learning rate: 1.0\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700 : 1.5662440645694733 learning rate: 1.0\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5800 : 1.571717188358307 learning rate: 1.0\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900 : 1.5699863839149475 learning rate: 1.0\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6000 : 1.5454582023620604 learning rate: 1.0\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      " and speopen and boingot to of the rinus in the by the casus of lock comprinced \n",
      "h pressions ovar most create make drimal liberia but the isoce allioghan a state\n",
      "ch mo inclatort illinate to the game lows more the zero ade willions the cody ny\n",
      "uge orgice capition of no spariam not xar becaus the an and amperave two three f\n",
      "ver sulsay the often one three eight nine eight frounided including of the hered\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6100 : 1.5581066513061523 learning rate: 1.0\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6200 : 1.5319100964069365 learning rate: 1.0\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300 : 1.5371161413192749 learning rate: 1.0\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6400 : 1.5357987654209138 learning rate: 1.0\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6500 : 1.5572090196609496 learning rate: 1.0\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6600 : 1.5915625846385957 learning rate: 1.0\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6700 : 1.5770351529121398 learning rate: 1.0\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6800 : 1.601893126964569 learning rate: 1.0\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6900 : 1.5820516741275787 learning rate: 1.0\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000 : 1.578736515045166 learning rate: 1.0\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "juer come author are living over royal served by divirorm function officially as\n",
      "by alide seckers war etchem both the pogna are relation first piraus thereformal\n",
      "jecically italia quen det resprince with way the leadened arts in the meta versi\n",
      "na blast and panulesna havela varlories and tim formative aegendtes on the popul\n",
      "er storinaty freeking d perbals is not commons expretedp lutwona two authors war\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step', step, ':', mean_loss, 'learning rate:', lr)\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
